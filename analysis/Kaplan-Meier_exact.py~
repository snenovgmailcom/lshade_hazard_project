#!/usr/bin/env python3
"""
Kaplan-Meier analysis with exact Binomial confidence intervals.
CORRECTED VERSION: p_cens computed over ALL generations, not just event times.
"""

import numpy as np
import pickle
from scipy.stats import beta
from pathlib import Path
import pandas as pd

# ============================================================
# 1. Clopper-Pearson exact CI for binomial proportion
# ============================================================

def clopper_pearson(d, n, alpha=0.05):
    """
    Exact binomial CI for proportion d/n.
    
    Parameters:
        d: number of successes (events)
        n: number of trials (exposure)
        alpha: significance level (default 0.05 for 95% CI)
    
    Returns:
        (lower, upper) bounds
    """
    if n == 0:
        return 0.0, 1.0
    if d == 0:
        h_L = 0.0
    else:
        h_L = beta.ppf(alpha/2, d, n - d + 1)
    if d == n:
        h_U = 1.0
    else:
        h_U = beta.ppf(1 - alpha/2, d + 1, n - d)
    return h_L, h_U


# ============================================================
# 2. Extract hitting times from convergence curves
# ============================================================

def extract_hitting_times(curves, f_star, epsilon):
    """
    Extract first-hitting times from convergence curves.
    
    Parameters:
        curves: list of convergence curves (best fitness per generation)
        f_star: known global minimum
        epsilon: tolerance
    
    Returns:
        numpy array of hitting times (np.inf for censored runs)
    """
    target = f_star + epsilon
    hit_times = []
    
    for curve in curves:
        curve = np.array(curve)
        hits = np.where(curve <= target)[0]
        if len(hits) > 0:
            hit_times.append(hits[0])
        else:
            hit_times.append(np.inf)
    
    return np.array(hit_times)


# ============================================================
# 3. Kaplan-Meier estimator with Greenwood CI
# ============================================================

def kaplan_meier_full(hit_times, alpha=0.05):
    """
    Full Kaplan-Meier analysis over ALL generations.
    
    Parameters:
        hit_times: array of hitting times (np.inf for censored)
        alpha: significance level
    
    Returns:
        List of dicts with t, n_t, d_t, h_hat, h_CI, S_hat, S_CI
    """
    hit_times = np.array(hit_times)
    R = len(hit_times)
    
    finite_hits = hit_times[hit_times < np.inf]
    if len(finite_hits) == 0:
        return None
    
    T = int(np.min(finite_hits))  # First hit time
    max_time = int(np.max(finite_hits))  # Last hit time
    
    results = []
    S = 1.0
    greenwood_sum = 0.0
    
    # Iterate over ALL generations from 0 to max_time
    for t in range(0, max_time + 1):
        # Number at risk just before time t
        n_t = int(np.sum(hit_times >= t))
        # Number of events at time t
        d_t = int(np.sum(hit_times == t))
        
        if n_t == 0:
            break
        
        # Hazard estimate and CI
        h_hat = d_t / n_t
        h_L, h_U = clopper_pearson(d_t, n_t, alpha)
        
        # Update survival estimate
        if d_t > 0:
            S = S * (1 - h_hat)
        
        # Greenwood's formula for variance
        if d_t > 0 and n_t > d_t:
            greenwood_sum += d_t / (n_t * (n_t - d_t))
        
        # CI for S using log-log transform
        if S > 0 and S < 1 and greenwood_sum > 0:
            se_log_log = np.sqrt(greenwood_sum) / abs(np.log(S))
            z = 1.96  # 95% CI
            S_L = max(0, S ** np.exp(z * se_log_log))
            S_U = min(1, S ** np.exp(-z * se_log_log))
        else:
            S_L, S_U = S, S
        
        results.append({
            't': t,
            'n_t': n_t,
            'd_t': d_t,
            'h_hat': h_hat,
            'h_lower': h_L,
            'h_upper': h_U,
            'S_hat': S,
            'S_lower': S_L,
            'S_upper': S_U,
        })
    
    return results


# ============================================================
# 4. Compute p_cens (MLE of constant hazard) - CORRECTED
# ============================================================

def compute_p_cens(hit_times, T=None, alpha=0.05):
    """
    Correct MLE of constant hazard rate after time T.
    
    IMPORTANT: Sums exposure over ALL generations from T to max_time,
    not just event times!
    
    Parameters:
        hit_times: array of hitting times
        T: start time (default: first hit)
        alpha: significance level
    
    Returns:
        dict with p_cens, CI, total_events, total_exposure
    """
    hit_times = np.array(hit_times)
    finite_hits = hit_times[hit_times < np.inf]
    
    if len(finite_hits) == 0:
        return None
    
    if T is None:
        T = int(np.min(finite_hits))
    
    max_time = int(np.max(finite_hits))
    
    total_events = 0
    total_exposure = 0
    
    # Sum over ALL generations from T to max_time
    for t in range(T, max_time + 1):
        # At risk at time t (not yet hit)
        n_t = int(np.sum(hit_times >= t))
        # Events at time t
        d_t = int(np.sum(hit_times == t))
        
        total_events += d_t
        total_exposure += n_t
    
    if total_exposure == 0:
        return None
    
    p_cens = total_events / total_exposure
    p_L, p_U = clopper_pearson(total_events, total_exposure, alpha)
    
    return {
        'p_cens': p_cens,
        'p_cens_L': p_L,
        'p_cens_U': p_U,
        'total_events': total_events,
        'total_exposure': total_exposure,
    }


# ============================================================
# 5. Compute a_valid with CI
# ============================================================

def compute_a_valid(hit_times, alpha=0.05):
    """
    Compute tightest valid exponential envelope rate with CI.
    
    Finds largest a such that:
        S(n) <= S(T-1) * (1-a)^(n-T+1) for all n >= T
    
    Parameters:
        hit_times: array of hitting times
        alpha: significance level
    
    Returns:
        dict with T, a_valid, a_valid_CI, binding_t
    """
    hit_times = np.array(hit_times)
    finite_hits = hit_times[hit_times < np.inf]
    
    if len(finite_hits) == 0:
        return None
    
    T = int(np.min(finite_hits))
    max_time = int(np.max(finite_hits))
    R = len(hit_times)
    
    # Compute Kaplan-Meier survival at each time
    S = 1.0
    greenwood_sum = 0.0
    S_T_minus_1 = 1.0  # S(T-1) = 1 since no hits before T
    
    a_max_list = []
    a_max_L_list = []
    a_max_U_list = []
    times_list = []
    
    for t in range(0, max_time + 1):
        n_t = int(np.sum(hit_times >= t))
        d_t = int(np.sum(hit_times == t))
        
        if n_t == 0:
            break
        
        h_hat = d_t / n_t
        
        if d_t > 0:
            S = S * (1 - h_hat)
            if n_t > d_t:
                greenwood_sum += d_t / (n_t * (n_t - d_t))
        
        # Only compute a_max for t >= T
        if t >= T:
            k = t - T + 1
            S_cond = S / S_T_minus_1
            
            if S_cond > 0 and S_cond < 1:
                a_max = 1 - S_cond ** (1/k)
                a_max_list.append(a_max)
                times_list.append(t)
                
                # CI via Greenwood
                if greenwood_sum > 0:
                    se_log_log = np.sqrt(greenwood_sum) / abs(np.log(S))
                    z = 1.96
                    S_L = max(1e-15, S ** np.exp(z * se_log_log))
                    S_U = min(1 - 1e-15, S ** np.exp(-z * se_log_log))
                    
                    S_cond_L = S_L / S_T_minus_1
                    S_cond_U = S_U / S_T_minus_1
                    
                    if S_cond_U > 0:
                        a_max_L_list.append(1 - S_cond_U ** (1/k))
                    else:
                        a_max_L_list.append(0.0)
                    
                    if S_cond_L > 0 and S_cond_L < 1:
                        a_max_U_list.append(1 - S_cond_L ** (1/k))
                    else:
                        a_max_U_list.append(1.0)
                else:
                    a_max_L_list.append(a_max)
                    a_max_U_list.append(a_max)
    
    if len(a_max_list) == 0:
        return None
    
    # Find minimum (binding constraint)
    idx_min = np.argmin(a_max_list)
    
    return {
        'T': T,
        'a_valid': min(a_max_list),
        'a_valid_L': min(a_max_L_list) if a_max_L_list else None,
        'a_valid_U': min(a_max_U_list) if a_max_U_list else None,
        'binding_t': times_list[idx_min],
        'max_time': max_time,
    }


# ============================================================
# 6. Analyze single function
# ============================================================

def analyze_function(hit_times, func_name, alpha=0.05, verbose=True):
    """
    Complete analysis for one function.
    """
    hit_times = np.array(hit_times)
    R = len(hit_times)
    n_hits = int(np.sum(hit_times < np.inf))
    
    if verbose:
        print(f"\n{'='*70}")
        print(f"Function: {func_name}")
        print(f"{'='*70}")
        print(f"Runs: {R}, Hits: {n_hits} ({100*n_hits/R:.1f}%)")
    
    # Need at least 3 hits for meaningful analysis
    if n_hits < 3:
        if verbose:
            print("Insufficient hits for analysis")
        return {
            'function': func_name,
            'R': R,
            'hits': n_hits,
            'hit_rate': n_hits / R,
            'T': None,
            'max_time': None,
            'median_tau': None,
            'p_cens': None,
            'p_cens_L': None,
            'p_cens_U': None,
            'a_valid': None,
            'a_valid_L': None,
            'a_valid_U': None,
            'ratio': None,
            'classification': 'Insufficient data',
        }
    
    finite_hits = hit_times[hit_times < np.inf]
    T = int(np.min(finite_hits))
    max_time = int(np.max(finite_hits))
    median_tau = np.median(finite_hits)
    
    if verbose:
        print(f"First hit T: {T}")
        print(f"Max hit time: {max_time}")
        print(f"Median τ (among hits): {median_tau:.0f}")
    
    # Compute p_cens
    p_result = compute_p_cens(hit_times, T, alpha)
    
    if verbose and p_result:
        print(f"\np_cens (MLE): {p_result['p_cens']:.6f}")
        print(f"  95% CI: [{p_result['p_cens_L']:.6f}, {p_result['p_cens_U']:.6f}]")
        print(f"  Total events: {p_result['total_events']}")
        print(f"  Total exposure: {p_result['total_exposure']}")
    
    # Compute a_valid
    a_result = compute_a_valid(hit_times, alpha)
    
    if verbose and a_result:
        print(f"\na_valid: {a_result['a_valid']:.6f}")
        print(f"  95% CI: [{a_result['a_valid_L']:.6f}, {a_result['a_valid_U']:.6f}]")
        print(f"  Binding at t = {a_result['binding_t']}")
    
    # Compute ratio and classify
    ratio = None
    classification = 'Unknown'
    
    if p_result and a_result and p_result['p_cens'] > 0:
        ratio = a_result['a_valid'] / p_result['p_cens']
        
        if ratio < 0.3:
            classification = 'Clustered'
        elif ratio < 0.7:
            classification = 'Moderate'
        else:
            classification = 'Geometric'
        
        if verbose:
            print(f"\na_valid / p_cens = {ratio:.4f}")
            print(f"Classification: {classification}")
    
    return {
        'function': func_name,
        'R': R,
        'hits': n_hits,
        'hit_rate': n_hits / R,
        'T': T,
        'max_time': max_time,
        'median_tau': median_tau,
        'p_cens': p_result['p_cens'] if p_result else None,
        'p_cens_L': p_result['p_cens_L'] if p_result else None,
        'p_cens_U': p_result['p_cens_U'] if p_result else None,
        'a_valid': a_result['a_valid'] if a_result else None,
        'a_valid_L': a_result['a_valid_L'] if a_result else None,
        'a_valid_U': a_result['a_valid_U'] if a_result else None,
        'ratio': ratio,
        'classification': classification,
    }


# ============================================================
# 7. Analyze all functions from PKL file
# ============================================================

def analyze_all_functions(pkl_path, epsilon=0.01, alpha=0.05, verbose=False):
    """
    Analyze all functions in a PKL file.
    """
    with open(pkl_path, 'rb') as f:
        data = pickle.load(f)
    
    results = []
    
    # Sort by function number
    func_names = sorted(data.keys(), key=lambda x: int(x.split('_f')[1]))
    
    for func_name in func_names:
        # Extract f_star from function name
        f_num = int(func_name.split('_f')[1])
        f_star = 100.0 * f_num
        
        # Get hitting times
        curves = [r['curve'] for r in data[func_name]]
        hit_times = extract_hitting_times(curves, f_star, epsilon)
        
        # Analyze
        result = analyze_function(hit_times, func_name, alpha, verbose)
        results.append(result)
    
    return pd.DataFrame(results)


# ============================================================
# 8. Print formatted summary table
# ============================================================

def print_summary_table(df, title="SUMMARY"):
    """Print nicely formatted summary table."""
    
    print(f"\n{'='*100}")
    print(f"{title}")
    print(f"{'='*100}")
    
    # Header
    print(f"{'Function':<14} {'Hits':>6} {'Rate':>7} {'T':>6} {'τ_med':>7} "
          f"{'p_cens':>10} {'p_cens 95% CI':>24} "
          f"{'a_valid':>10} {'a_valid 95% CI':>24} {'a/p':>8} {'Class':>12}")
    print("-" * 140)
    
    for _, row in df.iterrows():
        func = row['function'].replace('cec2017_', '')
        hits = f"{row['hits']}/{row['R']}"
        rate = f"{100*row['hit_rate']:.1f}%" if row['hit_rate'] is not None else "--"
        T = f"{row['T']:.0f}" if row['T'] is not None else "--"
        median_tau = f"{row['median_tau']:.0f}" if row['median_tau'] is not None else "--"
        
        if row['p_cens'] is not None:
            p_cens = f"{row['p_cens']:.6f}"
            p_ci = f"[{row['p_cens_L']:.6f}, {row['p_cens_U']:.6f}]"
        else:
            p_cens = "--"
            p_ci = "--"
        
        if row['a_valid'] is not None:
            a_valid = f"{row['a_valid']:.6f}"
            a_ci = f"[{row['a_valid_L']:.6f}, {row['a_valid_U']:.6f}]"
        else:
            a_valid = "--"
            a_ci = "--"
        
        ratio = f"{row['ratio']:.4f}" if row['ratio'] is not None else "--"
        classification = row['classification'] if row['classification'] else "--"
        
        print(f"{func:<14} {hits:>6} {rate:>7} {T:>6} {median_tau:>7} "
              f"{p_cens:>10} {p_ci:>24} "
              f"{a_valid:>10} {a_ci:>24} {ratio:>8} {classification:>12}")


# ============================================================
# 9. Generate LaTeX table for paper
# ============================================================

def generate_latex_table(df, caption="Kaplan--Meier hazard analysis"):
    """Generate LaTeX table for paper."""
    
    latex = []
    latex.append(r"\begin{table}[htbp]")
    latex.append(r"\centering")
    latex.append(r"\caption{" + caption + r"}")
    latex.append(r"\label{tab:km-results}")
    latex.append(r"\small")
    latex.append(r"\begin{tabular}{lrrrrccccl}")
    latex.append(r"\toprule")
    latex.append(r"Function & Hits & $T$ & $\tau_{\mathrm{med}}$ & "
                 r"$p_{\mathrm{cens}}$ & 95\% CI & "
                 r"$a_{\mathrm{valid}}$ & 95\% CI & $a/p$ & Class \\")
    latex.append(r"\midrule")
    
    for _, row in df.iterrows():
        if row['hits'] < 3:
            continue
        
        func = row['function'].replace('cec2017_f', 'F')
        hits = row['hits']
        T = int(row['T']) if row['T'] is not None else "--"
        median_tau = int(row['median_tau']) if row['median_tau'] is not None else "--"
        
        # Format p_cens
        if row['p_cens'] is not None:
            if row['p_cens'] < 0.001:
                p_cens = f"${row['p_cens']:.2e}$"
                p_ci = f"[{row['p_cens_L']:.2e}, {row['p_cens_U']:.2e}]"
            else:
                p_cens = f"{row['p_cens']:.4f}"
                p_ci = f"[{row['p_cens_L']:.4f}, {row['p_cens_U']:.4f}]"
        else:
            p_cens = "--"
            p_ci = "--"
        
        # Format a_valid
        if row['a_valid'] is not None:
            if row['a_valid'] < 0.001:
                a_valid = f"${row['a_valid']:.2e}$"
                a_ci = f"[{row['a_valid_L']:.2e}, {row['a_valid_U']:.2e}]"
            else:
                a_valid = f"{row['a_valid']:.4f}"
                a_ci = f"[{row['a_valid_L']:.4f}, {row['a_valid_U']:.4f}]"
        else:
            a_valid = "--"
            a_ci = "--"
        
        ratio = f"{row['ratio']:.3f}" if row['ratio'] is not None else "--"
        classification = row['classification']
        
        latex.append(f"{func} & {hits} & {T} & {median_tau} & "
                     f"{p_cens} & {p_ci} & "
                     f"{a_valid} & {a_ci} & {ratio} & {classification} \\\\")
    
    latex.append(r"\bottomrule")
    latex.append(r"\end{tabular}")
    latex.append(r"\end{table}")
    
    return "\n".join(latex)


# ============================================================
# 10. Main
# ============================================================

if __name__ == "__main__":
    
    print("=" * 100)
    print("KAPLAN-MEIER ANALYSIS WITH EXACT 95% CONFIDENCE INTERVALS")
    print("=" * 100)
    print("\nBased on Binomial structure (valid due to exchangeability of i.i.d. runs)")
    print("- p_cens CI: Clopper-Pearson exact binomial")
    print("- S(t) CI: Greenwood's formula + log-log transformation")
    print("- a_valid CI: propagated from S(t) bounds")
    
    # Paths
    pkl_100k = Path("experiments/r_lshade_D10_nfev_100000/raw_results_lshade.pkl")
    pkl_1M = Path("experiments/r_lshade_D10_nfev_1000000/raw_results_lshade.pkl")
    
    epsilon = 0.01
    alpha = 0.05
    
    # ---- 100k analysis ----
    if pkl_100k.exists():
        print("\n" + "=" * 100)
        print("NFEV = 100,000")
        print("=" * 100)
        
        df_100k = analyze_all_functions(pkl_100k, epsilon, alpha, verbose=False)
        print_summary_table(df_100k, "RESULTS: NFEV = 100,000")
        
        # Save
        df_100k.to_csv("results/km_analysis_100k_CI.csv", index=False)
        print("\nSaved: results/km_analysis_100k_CI.csv")
        
        # Classification summary
        print("\n--- Classification Summary (100k) ---")
        for cls in ['Clustered', 'Moderate', 'Geometric', 'Insufficient data']:
            count = len(df_100k[df_100k['classification'] == cls])
            funcs = df_100k[df_100k['classification'] == cls]['function'].tolist()
            funcs_short = [f.replace('cec2017_f', 'F') for f in funcs]
            print(f"  {cls}: {count} functions - {', '.join(funcs_short)}")
    
    # ---- 1M analysis ----
    if pkl_1M.exists():
        print("\n" + "=" * 100)
        print("NFEV = 1,000,000")
        print("=" * 100)
        
        df_1M = analyze_all_functions(pkl_1M, epsilon, alpha, verbose=False)
        print_summary_table(df_1M, "RESULTS: NFEV = 1,000,000")
        
        # Save
        df_1M.to_csv("results/km_analysis_1M_CI.csv", index=False)
        print("\nSaved: results/km_analysis_1M_CI.csv")
        
        # Classification summary
        print("\n--- Classification Summary (1M) ---")
        for cls in ['Clustered', 'Moderate', 'Geometric', 'Insufficient data']:
            count = len(df_1M[df_1M['classification'] == cls])
            funcs = df_1M[df_1M['classification'] == cls]['function'].tolist()
            funcs_short = [f.replace('cec2017_f', 'F') for f in funcs]
            print(f"  {cls}: {count} functions - {', '.join(funcs_short)}")
    
    # ---- Combined analysis (use 1M where available, else 100k) ----
    if pkl_100k.exists() and pkl_1M.exists():
        print("\n" + "=" * 100)
        print("COMBINED RESULTS (1M where available, else 100k)")
        print("=" * 100)
        
        # Merge: prefer 1M data
        df_combined = df_100k.copy()
        
        for idx, row in df_1M.iterrows():
            func = row['function']
            # Replace if 1M has more hits or better data
            mask = df_combined['function'] == func
            if mask.any():
                if row['hits'] >= df_combined.loc[mask, 'hits'].values[0]:
                    df_combined.loc[mask] = row
            else:
                df_combined = pd.concat([df_combined, pd.DataFrame([row])], ignore_index=True)
        
        # Sort
        df_combined = df_combined.sort_values(
            by='function', 
            key=lambda x: x.str.extract(r'f(\d+)')[0].astype(int)
        ).reset_index(drop=True)
        
        print_summary_table(df_combined, "COMBINED RESULTS")
        
        # Save
        df_combined.to_csv("results/km_analysis_combined_CI.csv", index=False)
        print("\nSaved: results/km_analysis_combined_CI.csv")
        
        # Generate LaTeX
        latex = generate_latex_table(df_combined, 
            caption="Kaplan--Meier hazard analysis on CEC2017 ($D=10$, $R=51$ runs, "
                    "$\\varepsilon=10^{-2}$). Functions with $<3$ hits omitted.")
        
        with open("results/km_table.tex", "w") as f:
            f.write(latex)
        print("Saved: results/km_table.tex")
        
        # Final summary
        print("\n" + "=" * 100)
        print("INTERPRETATION")
        print("=" * 100)
        print("""
